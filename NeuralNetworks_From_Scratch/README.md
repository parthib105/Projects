# ðŸ§  Neural Network from Scratch (on MNIST)

This project implements a **simple neural network from scratch** using NumPy, trained on the MNIST dataset (handwritten digits).

## ðŸ“¦ Project Structure

```text
â”œâ”€â”€ data/ # MNIST data stored here
â”œâ”€â”€ main.py # Entry-point for training
â”œâ”€â”€ neural_network/ # Core neural net components
â”‚ â”œâ”€â”€ activation_functions.py
â”‚ â”œâ”€â”€ loss_functions.py
â”‚ â”œâ”€â”€ model.py
â”‚ â””â”€â”€ init.py
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # You're reading it!
```


## ðŸš€ How to Run

```bash
pip install -r requirements.txt
python main.py

ðŸ§  Features
- Manual implementation of:
    - ReLU and Softmax
    - Cross-Entropy Loss
    - Backpropagation and Gradient Descent
- MNIST dataset loading using torchvision
- Training loop from scratch


âœ… TODO
- Add accuracy metric
- Add support for other datasets
- Add visualization of weights/loss